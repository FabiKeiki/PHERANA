configfile: "../config/config.yaml"

#################### SET UP Variables #######################################
import os
import pandas as pd
import numpy as np
from scripts.snake_functions import *

rule all:
    input:
        #directory("../results/phages/vcontact")
        "../data/bacteria/bins"
        #outdir = directory("../scratch_link/binning/bins")

#######################################################
########################### DATA VALIDATION ####################################
################################################################################
# The following pipeline is used to ascertain that the metagenomics samples
# are composed of what we expect (viruses in the virome fraction, bacteria in the
# bacteriome one)
################################################################################
################################################################################


################################### Concat Lanes ###################################


#Concatenate raw reads from the same sample that were sequenced on different lanes
rule concat_lanes:
    input:
        R1s=get_direction_r1,
        R2s=get_direction_r2
    output:
        R1_concat="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz", # these files go in scratch because they can be quickly recreated if needed
        R2_concat="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz"
    threads: 2
    log:
        "logs/data_validation/lane_concatenation/{sample}_concat.log"
    resources:
        account = "pengel_beemicrophage",
        runtime_s = 7200
    shell:
        "cat {input.R1s} > {output.R1_concat}; cat {input.R2s} > {output.R2_concat}; "


################################### Kraken2 ###################################

# This rule takes the concatenated raw reads files and runs them against the krakend db
rule run_kraken:
    input:
        R1="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        R2="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz",
        db="../../../mndiaye1/PHOSTER/workflow/resources/default_DBs/230228_costum_kraken2db_new"
    output:
        tab=temp("../results/data_validation/kraken2_output/{sample}_kraken2_report.kraken"),
        rep="../results/data_validation/kraken2_output/Reports/{sample}_kraken2_report",
    conda:
        "envs/Kraken2.yaml"
    threads: 24
    log:
        "logs/data_validation/kraken2/run/{sample}_kraken2.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime_s= 10800
    shell:
        "kraken2 --use-names --threads {threads} \
         --db {input.db} \
         --fastq-input --report {output.rep}  --gzip-compressed \
         --paired {input.R1} {input.R2} \
         > {output.tab}"
    
# This rule parses the kraken output for further analyes
rule parse_kraken_report:
    input:
        "scripts/data_validation/parse_kraken_report.py", # if you change the script, the rule runs again
        expand("../results/data_validation/kraken2_output/Reports/{sample}_kraken2_report",sample=config["samples"])
    output:
        "../results/data_validation/kraken2_output/Summary/all_samples_report.txt"
    threads: 2
    params:
        "../results/data_validation/kraken2_output/"
    log:
        "logs/data_validation/kraken2/parsing/report_parser_kraken2.log"
    resources:
        account = "pengel_beemicrophage",
        runtime_s= 1000
    script:
        "scripts/data_validation/parse_kraken_report.py"



############################# QC and Trimming ##################################

# This rule does a fastQC on the raw reads
rule fastQC_PreTrimming:
    input:
        R1=get_direction_r1,
        R2=get_direction_r2
    output:
        temp(directory("../results/data_validation/QC/preTrimming/QC_{sample}/"))
    threads: 2
    log:
        "logs/data_validation/QC/{sample}_QC.log"
    conda:
        "envs/fastqc.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime_s= 3600
    shell:
        "mkdir -p {output}; "
        "fastqc -o {output} {input.R1};"
        "fastqc -o {output} {input.R2};"


# This rule runs the trimming of the raw reads
rule rawreads_trimming:
    input:
        R1="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        R2="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz"
    output:
        R1_paired="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2_paired="../data/trimmed_reads/{sample}_R2_paired.fastq.gz",
        R1_unpaired="../data/trimmed_reads/{sample}_R1_unpaired.fastq.gz",
        R2_unpaired="../data/trimmed_reads/{sample}_R2_unpaired.fastq.gz"
    threads: 8
    params:
        nextera="../resources/reference_assemblies/short_RefSeqs/NexteraPE-PE.fa",
        q=28,
        min_length=40
    log:
        "logs/data_validation/trimming/{sample}_trimming.log"
    conda:
        "envs/trimmomatic.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime_s= 10800
    shell:
        "trimmomatic PE -phred33 -threads {threads} {input.R1} {input.R2} \
         {output.R1_paired} {output.R1_unpaired} {output.R2_paired} {output.R2_unpaired} \
         ILLUMINACLIP:{params.nextera}:2:30:10 \
         LEADING:{params.q} TRAILING:{params.q} MINLEN:{params.min_length}"

# this rule does a post trimming fast QC
rule fastQC_PostTrimming:
    input:
        R1="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2="../data/trimmed_reads/{sample}_R2_paired.fastq.gz"
    output:
        temp(directory("../results/data_validation/QC/postTrimming/QC_{sample}/"))
    threads: 2
    log:
        "logs/data_validation/QC/{sample}_postQC.log"
    conda:
        "envs/fastqc.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime_s= 3600
    shell:
        "mkdir -p {output}; "
        "fastqc -o {output} {input.R1} {input.R2}"

rule parse_fastQC:
    input:
        preT=expand("../results/data_validation/QC/preTrimming/QC_{sample}/", sample=config["samples"]),
        postT=expand("../results/data_validation/QC/postTrimming/QC_{sample}/", sample=config["samples"])
    output:
        "../results/data_validation/QC/Summary/fastQC_summary.txt"
    threads: 2
    log:
        "logs/data_validation/QC/summarize_fastQC.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime= 1800
    script:
        "scripts/data_validation/parse_fastqc_output.py"


############################### Host Filtering #################################

# This rule uses bbsplit to map remove reads from the honeybee genome and/or the human genomes
rule host_filtering:
    input:
        R1="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2="../data/trimmed_reads/{sample}_R2_paired.fastq.gz"
    output:
        dir=temp(directory("../results/data_validation/host_filtering/discarded/{sample}_discarded/")), # I don't need the sam of the mapping so I delete them immediatly
        unmapped_R1="../data/host_filtered_reads/{sample}_R1_HF.fastq.gz", # these are the filtered reads
        unmapped_R2="../data/host_filtered_reads/{sample}_R2_HF.fastq.gz",
        refstats="../results/data_validation/host_filtering/HF_mappings_stats/{sample}_refstats.out"
    conda:
        "envs/bwa_mapping.yaml"
    threads: 25
    params:
        ref_Acer="../resources/reference_assemblies/A_cerana/GCF_001442555.1_ACSNU-2.0_genomic.fna.gz",
        ref_Hsap="../resources/reference_assemblies/H_sapiens/GCF_000001405.40_GRCh38.p14_genomic.fna.gz",
        xmx="50g"
    log:
        "logs/data_validation/HF/{sample}_HF.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime_s= 28800
    shell:
        "bbsplit.sh in1={input.R1} in2={input.R2} ref={params.ref_Acer},{params.ref_Hsap} \
        basename={output.dir}/{wildcards.sample}_HF_discarded_%.sam \
        refstats={output.refstats} rebuild=t nodisk=t \
        outu1={output.unmapped_R1} outu2={output.unmapped_R2} nzo=f -Xmx{params.xmx} threads={threads}"


# This rule parses the refstats output of the filering
rule parse_filtering_refstats:
    input:
        files=expand("../results/data_validation/host_filtering/HF_mappings_stats/{sample}_refstats.out", sample=config["samples"])
    output:
        "../results/data_validation/host_filtering/HF_mappings_stats/HF_refstats.txt"
    threads: 2
    log:
        "logs/data_validation/HF/HF_refstats_parsing.log"
    params:
        file1="../results/data_validation/host_filtering/HF_mappings_stats/file1.txt",
        file2="../results/data_validation/host_filtering/HF_mappings_stats/file2.txt",
        tmp="../results/data_validation/host_filtering/HF_mappings_stats/tmp.txt",
        sams=expand("{sample}", sample=config["samples"])
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500,
        runtime_s= 1800
    shell:
        "echo -e 'sample\tname\tperc_unambiguousReads\tunambiguousMB\tperc_ambiguousReads\tambiguousMB\tunambiguousReads\tambiguousReads\tassignedReads\tassignedBases' > {output}; "
        "tail -n +2 -q {input.files} > {params.file1}; "
        "printf '%s\n' {params.sams} > {params.file2}; "
        "awk '{{for(i=0;i<2;i++)print}}' {params.file2} > {params.tmp}; "
        "paste -d '\t' {params.tmp} {params.file1} >> {output}"


############################### count_reads ##################################
# After trimming and host filtering the reads, I wanna know how much I lost in
# Terms of reads and bases


# this rules returns a table of read count before and after trimming
rule count_reads_qc:
    input:
        preT="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        postT="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        postF="../data/host_filtered_reads/{sample}_R1_HF.fastq.gz"
    output:
        temp("../results/data_validation/QC/{sample}_read_count.txt")
    log:
        "logs/data_validation/QC/read_count/{sample}_read_count.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 4000,
        runtime_s= 1800
    shell:
        "touch {output}; "
        "(./scripts/data_validation/count_reads.sh {input.preT} {input.postT} {input.postF} {output})2>{log}"


rule count_reads_summary:
    input:
        sams=expand("../results/data_validation/QC/{sample}_read_count.txt", sample=config["samples"])
    output:
        "../results/data_validation/QC/Summary/read_count.txt"
    log:
        "logs/data_validation/QC/read_count/summary_read_count.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 4000,
        runtime_s= 1800
    shell:
        "(awk 'FNR!=NR && FNR==1 {{next}} 1' {input.sams} > {output})2>{log}"



################################################################################
###############################  mOTUS  ########################################
################################################################################
# mOTUs is more has more taxonomical resolution than kraken, so once the reads
# are all cleaned and filtered, I run mOTUs to obtain a genus-level composition
# of the remaining reads (for the bacterial samples)
################################################################################
################################################################################

# this rule runs the motu profiling
# it is better to launch this rule with one sample first and then all the others,
# mOTUs db download doesn't handle weell multiple files trying to download it and
# access it at the same time and the jobs might fail.
rule run_motus:
    input:
        reads1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        reads2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        motus_temp = temp("../results/data_validation/motus_output/map/{sample}_map.motus")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime_s= 9000
    threads: 24
    log:
        "logs/data_validation/motus/{sample}_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus downloadDB; " # just leave it here to be safe - it warns and continues
        "motus profile -f {input.reads1} -r {input.reads2} -n {wildcards.sample} -o {output.motus_temp} -t {threads}"

# this roule run the mouts counting of the marker genes that map to the datbase
rule run_motus_count:
    input:
        reads1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        reads2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        motus_temp = temp("../results/data_validation/motus_output/count/{sample}_count.motus")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime_s= 9000
    threads: 24
    log:
        "logs/data_validation/motus/{sample}_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus downloadDB; " # just leave it here to be safe - it warns and continues
        "motus profile -f {input.reads1} -r {input.reads2} -c -n {wildcards.sample} -o {output.motus_temp} -t {threads}"

rule merge_motus:
    input:
        motus_temp = expand("../results/data_validation/motus_output/map/{sample}_map.motus", sample=config["samples"]),
        motus_count= expand("../results/data_validation/motus_output/count/{sample}_count.motus", sample=config["samples"])
    output:
        motus_merged = "../results/data_validation/motus_output/Summary/samples_merged_map.motus",
        mouts_merged_count = "../results/data_validation/motus_output/Summary/samples_merged_count.motus"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 500000,
        runtime= 7200
    threads: 12
    log:
        "logs/data_validation/motus/merge_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus merge -a bee -i $(echo \"{input.motus_temp}\" | sed -e 's/ /,/g' ) > {output.motus_merged}; "
        "motus merge -a bee -c -i $(echo \"{input.motus_count}\" | sed -e 's/ /,/g' ) > {output.mouts_merged_count}"

rule parse_motus:
    input:
        motus_tab = "../results/data_validation/motus_output/Summary/samples_merged_map.motus",
        motus_count = "../results/data_validation/motus_output/Summary/samples_merged_count.motus"
    output:
        "../results/data_validation/motus_output/Summary/motus_combined.txt"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 50000,
        runtime_s= 14400
    log:
        "logs/data_validation/motus/parse_motus.log"
    conda:
        "envs/base_R_env.yaml"
    script:
        "scripts/data_validation/parse_motus.R"



################################################################################
################################## Sample Assembly #############################
################################################################################
# This part of the pipleine takes takes host filtered reads (both virome and bacterial) and reconstructs MAGS
# This part of the pipeline is divided in several steps:
#   (1): Assembly : Using different assemblers and comparing them with quast
#       (1.1) metaSPades
#       (1.2) metaviralSPades
#       (1.3) stats on assemblies with metquast
#       (1.4) filtering assemblies
#       (1.5) stats on filtered assemblies with metaquast
################################################################################
################################################################################

##################################### (1.1) ######################################
# the following rules use metaSPades to assemble the metagenomes of every sample.

rule assemble_metaspades:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scratch_dir=directory("../scratch_link/assembly/metaspades/{sample}_metaspades/"),
        contigs="../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs.fasta"
    params:
        memory_limit = 200,
    threads: 40
    resources:
        account="pengel_beemicrophage",
        mem_mb= 250000,
        runtime_s= 25000
    log:
        "logs/assembly/metaspades/{sample}_assemble_HF.log"
    conda:
        "envs/assembly.yaml"
    shell:
        "spades.py --meta --pe1-1 {input.R1} --pe1-2 {input.R2} \
        -o {output.scratch_dir} \
        -k 21,33,55,77,99,127 -m {params.memory_limit} -t {threads}; "
        "mv {output.scratch_dir}/contigs.fasta {output.contigs}; "
        

##################################### (1.2) ######################################
# the following rules use metaviralSPades to assemble the metagenomes of every sample.

rule assemble_metaviralspades:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scratch_dir=directory("../scratch_link/assembly/metaviralspades/{sample}_metaviralspades/"),
        contigs="../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs.fasta"
    params:
        memory_limit = 200
    threads: 40
    resources:
        account="pengel_beemicrophage",
        mem_mb= 250000,
        runtime_s= 25000
    log:
        "logs/assembly/metaviralspades/{sample}_assemble.log"
    conda:
        "envs/assembly.yaml"
    shell:
        "spades.py --metaviral --pe1-1 {input.R1} --pe1-2 {input.R2} \
        -o {output.scratch_dir} \
        -k 21,33,55,77,99,127 -m {params.memory_limit} -t {threads}; "
        "mv {output.scratch_dir}/contigs.fasta {output.contigs}; "
        
        



##################################### (1.3) ######################################
# the following rules use metaquast to retrieve statistics for each assemblers


rule run_quast_contigs:
    input:
        mspades= "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs.fasta",
        mvspades= "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs.fasta",
    output:
        dir=directory("../scratch_link/assembly/quast/{sample}_quast_output/"),
        report= "../results/assembly/quast/{sample}_quast_report.tsv"
        
    log:
        "logs/assembly/quast/{sample}_quast.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 25000,
        runtime_s =  3600
    conda:
        "envs/quast_contigs.yaml"
    shell:
        "metaquast.py {input.mspades} {input.mvspades} -o {output.dir} --max-ref-number 0;"
        "mv {output.dir}/transposed_report.tsv {output.report}"


rule parse_quast_reports:
    input:
       assembly =expand("../results/assembly/quast/{sample}_quast_report.tsv",sample=config["samples"])
    output:
        "../results/assembly/quast/summary_quast_report.tsv"
    log:
        "logs/assembly/quast/summary_quast.log"
    resources:
        account="pengel_beemicrophage",
        runtime_s = 1800
    script:
        "scripts/assembly/parse_quast_report.py"

##################################### (1.5) ######################################
# THe follwoing rules apply a filter to the assembled contigs

# rule filter_assembly:
#     input:
#         expand("../data/assembly/{assembly}/{sample}_{assembly}/{sample}_{assembly}_contigs.fasta",assembly=config["assembly"],sample=config["samples"])
#     output:
#         "../data/assembly/{assembly}/{sample}_{assembly}/{sample}_{assembly}_contigs_filt.fasta"
#     params:
#         length_t = 1000,
#         cov_t = 1
#     resources:
#         account="pengel_beemicrophage",
#         mem_mb= 2000,
#         runtime_s= 1800
#     log:
#         "logs/assembly/filtering/{assembly}/{sample}_filtering.log"
#     conda:
#         "envs/biopython.yaml"
#     script:
#         "scripts/assembly/filter_assembly.py"
#################################### (1.5) ######################################
# THe follwoing rules apply a filter to the assembled contigs


rule filter_metaspades:
    input:
        "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs.fasta"
    output:
        "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs_filt.fasta"
    params:
        length_t = 1000,
        cov_t = 1
    resources:
        account="pengel_beemicrophage",
        mem_mb= 2000,
        runtime_s= 1800
    log:
        "logs/assembly/filtering/metaspades/{sample}_filtering.log"
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/assembly/filter_assembly.py"

rule filter_metaviralspades:
    input:
        "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs.fasta"
    output:
        "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs_filt.fasta"
    params:
        length_t = 1000,
        cov_t = 1
    resources:
        account="pengel_beemicrophage",
        mem_mb= 2000,
        runtime_s= 1800
    log:
        "logs/assembly/filtering/metaviralspades/{sample}_filtering.log"
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/assembly/filter_assembly.py"

##################################### (1.6) ######################################
# the following rules use metaquast to retrieve statistics for each filtered assemblies


rule run_quast_contigs_filt:
    input:
        mspades= "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs_filt.fasta",
        mvspades= "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs_filt.fasta"
    output:
        dir=directory("../scratch_link/assembly/quast/{sample}_quast_output_filt/"),
        report= "../results/assembly/quast/{sample}_quast_report_filt.tsv"
    log:
        "logs/assembly/quast/{sample}_quast_filt.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 25000,
        runtime_s =  3600
    conda:
        "envs/quast_contigs.yaml"
    shell:
        "metaquast.py {input.mspades} {input.mvspades} -o {output.dir} --max-ref-number 0;"
        "mv {output.dir}/transposed_report.tsv {output.report}"



rule parse_quast_reports_filt:
    input:
       expand("../results/assembly/quast/{sample}_quast_report_filt.tsv",sample=config["samples"])
    output:
        "../results/assembly/quast/summary_quast_report_filt.tsv"
    log:
        "logs/assembly/quast/summary_quast_filt.log"
    resources:
        account="pengel_beemicrophage",
        runtime_s = 1800
    script:
        "scripts/assembly/parse_quast_report.py"

rule concat_assembly:
    input: 
        mspades= "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs_filt.fasta",
        mvspades= "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs_filt.fasta",
    output:
        concat_assembly = "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    log:
        "logs/assembly/concat_assembly/{sample}_concat.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 1000,
        runtime_s = 7200
    threads:
        5
    shell:
        "cat {input.mspades} {input.mvspades} > {output.concat_assembly}"

################################################################################
############################## Phage identification  ###########################
################################################################################
#This part of the pipeline runs four tools on all the assembly, to identify 
#phages, these sequences will then dereplicated and binned. Each phages id tools
#ouput are parsed and merged, a confidence score is attributed to each id
#and a fasta file of the phages sequences is extracted :
#   (1): PHAGES id tools
#       (1.1) run phage id tools (virstorter, viralverify, deepvirfinder, vibrant)
#   (2) : Parsing outputs
#       (2.1) parsing tools
#       (2.2) summary of tools



##################################### (1.1) ######################################

rule run_virsorter:
    input:
       "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = temp(directory("../scratch_link/identification/virsorter/{sample}_virsorter/")),
        score = "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score.tsv/"
    params:
        db = directory("../scratch_link/virsorter/")
    log:
        "logs/identification/virsorter/{sample}_virsorter.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/virsorter2.yaml"
    shell:
        "virsorter setup -d {params.db} -j 40;"
        "virsorter run -w {output.outdir} -i {input} --keep-original-seq  -j 40 all;"
        "mv {output.outdir}/final-viral-score.tsv {output.score}"

rule run_viralverify:
    input:
        "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = temp(directory("../scratch_link/identification/viralverify/{sample}_viralverify/")),
        score = "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score.csv/"
    params:
        hmm = "../resources/hmms/nbc_hmms.hmm.gz"
    log:
        "logs/identification/viralverify/{sample}_viralverify.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/viralverify.yaml"
    shell:
        "viralverify -f {input} -o {output.outdir} --hmm {params.hmm} -t {threads};"
        "mv {output.outdir}/{wildcards.sample}_concat_assembly_result_table.csv {output.score}"

rule run_deepvirfinder:
    input:
         "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = temp(directory("../scratch_link/identification/deepvirfinder/{sample}_deepvirfinder")),
        score = "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score.tsv/"
    log:
        "logs/identification/deepvirfinder/{sample}_dvfinder.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/deepvirfinder.yaml"
    shell:
        "dvf.py -i {input} -o {output} -c {threads};"
        "mv {output.outdir}/{wildcards.sample}_concat_assembly.fasta_gt1bp_dvfpred.txt {output.score}"

rule download_vibrant_db:
    output:
        temp(directory("vibrant_db"))
    log:
        "logs/identification/vibrant/download_db.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/vibrant.yaml"
    shell:
        "download-db.sh;"
        "mkdir {output}"

rule run_vibrant:
    input:
        assembly = "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta",
        vibrant_db = "vibrant_db"
    output:
        outdir = temp(directory("../scratch_link/identification/vibrant/{sample}/")),
        score = "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score.tsv/"
    log:
        "logs/identification/vibrant/{sample}_vibrant.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/vibrant.yaml"
    shell:
        "VIBRANT_run.py -i {input.assembly} -folder {output.outdir} -t {threads};"
        "mv {output.outdir}/VIBRANT_{wildcards.sample}_concat_assembly/VIBRANT_phages_{wildcards.sample}_concat_assembly/{wildcards.sample}_concat_assembly.phages_combined.txt {output.score};"

##################################### (2.1) ######################################

rule parse_virsorter:
    input:
        "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score.tsv/"
    output:
        "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score_parsed.tsv/"
    log:
        "logs/identification/virsorter/{sample}_parse_virsoter.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_virsorter.py"

rule parse_viralverify:
    input:
        "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score.csv/"
    output:
        "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score_parsed.tsv/"
    log:
        "logs/identification/viralverify/{sample}_parse_viralverify.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_viralverify.py"

rule parse_deepvirfinder:
    input:
        "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score.tsv/"
    output:
        "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score_parsed.tsv/"
    log:
        "logs/identification/deepvirfinder/{sample}_parse_deepvirfinder.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_deepvirfinder.py"

rule parse_vibrant:
    input:
        "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score.tsv/"
    output:
        "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score_parsed.tsv/"
    log:
        "logs/identification/vibrant/{sample}_parse_vibrant.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_vibrant.py"

##################################### (2.2) ######################################

rule parse_phage_id:
    input:
        virsorter = "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score_parsed.tsv/",
        viralverify =  "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score_parsed.tsv/",
        deepvirfinder =  "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score_parsed.tsv/",
        vibrant =  "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score_parsed.tsv/"
    output:
        "../results/identification/summary/{sample}_summary_score.tsv/"
    log:
        "logs/identification/summary/{sample}_summary.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        2
    script:
        "scripts/identification/parse_phage_id.py"

rule concat_phage_id:
    input:
        expand("../results/identification/summary/{sample}_summary_score.tsv/",sample=config['samples'])
    output:
        "../results/identification/summary/all_sample_summary_score.tsv/"
    log:
        "logs/identification/summary/all_sample_summary.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        2
    shell:
        "cat {input} > {output}"




###############################################################################
################################# Binning #####################################
###############################################################################
#This part bins the contigs using VAMB. Vamb is then parsed throug PHAMB. 
#   (1) Vamb
#   (2) Phamb
###############################################################################

######################################(1) Vamb ###############################

rule concat_concat_assembly:
    input:
        refs= expand("../data/assembly/concat_assembly/{sample}_concat_assembly.fasta",sample=config['samples'])
    output:
        ref = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"
    conda:
        "envs/v_binning.yaml"
    threads: 1
    log:
        "logs/binning/concat_assembly/concat.log"
    resources:
        account= "pengel_beemicrophage",
        mem_mb= 20000,
        runtime= "3600"
    shell:
        "python scripts/binning/concatenate_viral_assemblies.py {output.ref} {input.refs}"


rule build_mapping_assembly_index:
    input:
        ref = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"
    output:
        directory("../data/mapping/concat_assembly_index/")
    conda:
        "envs/map_env.yaml"
    threads: 15
    params:
        basename="assembly_index"
    log:
        "logs/binning/mapping/indexing/concat_assembly.log"
    resources:
        account= "pengel_beemicrophage",
        mem_mb= 20000,
        runtime= "3600"
    shell:
        "mkdir -p {output};"
        "bowtie2-build {input} {output}/{params.basename} --threads {threads}"

rule mapping_viral_db: 
    input:
        index = "../data/mapping/concat_assembly_index",
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        sam=temp("../scratch_link/binning/mapping/{sample}/{sample}_mapped_to_assembly.sam")
    params:
        basename="assembly_index"
    resources:
        account="pengel_beemicrophage",
        runtime="36000",
        mem_mb = 10000
    threads: 15
    conda: "envs/map_env.yaml"
    log:
        "logs/binning/mapping/map/{sample}_mapped_to_assembly.log"
    benchmark: 
        "logs/binning/mapping/map/{sample}_mapped_to_assembly.benchmark"
    shell:
        "bowtie2 -x {input.index}/{params.basename} -1 {input.R1} -2 {input.R2} -S {output.sam} --threads {threads}"



rule sort_virome_backmapping_bam:
    input:
        sam="../scratch_link/binning/mapping/{sample}/{sample}_mapped_to_assembly.sam"
    output:
        bam= temp("../scratch_link/binning/mapping/{sample}/{sample}_mapped_to_assembly.bam"),
        sorted_bam="../scratch_link/binning/mapping/{sample}/{sample}_mapped_to_assembly_sorted.bam"
    log:
        "logs/binning/mapping/sortingbams/sort_bam_{sample}.log"
    conda:
        "envs/map_env.yaml"
    threads: 5
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 8000,
        runtime= "5400"
    shell:
        "samtools view -bh {input.sam} > {output.bam};"
        "samtools sort {output.bam} -@ {threads} -o {output.sorted_bam}"
    

rule run_vamb:
    input:
        ref = "../data/assembly/concat_assembly/all_concat_assemblies.fasta",
        sorted_bam=expand("../scratch_link/binning/mapping/{sample}/{sample}_mapped_to_assembly_sorted.bam", sample=config["samples"])
    output:
        directory("../results/binning/vamb/")
    log:
        "logs/binning/vamb/binning.log"
    conda:
        "envs/v_binning.yaml"
    threads: 25
    params:
        min_contig_l=2000
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime= "14400"
    shell:
        "vamb --outdir {output} --fasta {input.ref} --bamfiles {input.sorted_bam} -m {params.min_contig_l} -p {threads} -o C"

                                                           #

rule parse_vamb:
    input:
        "../results/binning/vamb/vae_clusters.tsv"
    output:
        vamb_parsed = "../results/binning/vamb/vamb_parsed.tsv"
    params:
        directory("../data/assembly/concat_assembly/")
    log:
        "logs/binning/parse_vamb/parse_vamb.log"
    conda:
        "envs/biopython.yaml"
    threads: 10
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "72000"
    script:
        "scripts/binning/parse_vamb.py"





######################################(2) PHAMB ###############################

MICOMPLETEDB = "../resources/hmms/Bact105.hmm"
VOGDB= "../resources/hmms/AllVOG.hmm"
DVFDIR = "../resources/tools/DeepVirFinder"
TMP_DIR = "../scratch_link/mag_annotation_tmp"
CONTIGSUFFIX = '.fna'
ASSEMBLY = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"


checkpoint split_vamb_contigs:
    input:
        contigs = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"
    output:
        asmbl= directory("../results/binning/phamb/assembly/"),
        contigs="../results/binning/phamb/contigs.npz",
        length="../results/binning/phamb/contig_lengths.npz",
        sam_tab="../results/binning/phamb/sample_table.txt"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 8000,
        runtime= "1800"
    params:
        out="../results/binning/phamb/",
        asmbl= "assembly",
        contigs="contigs.npz",
        length="contig_lengths.npz",
        sam_tab="sample_table.txt"
    threads: 1
    conda:
        "envs/v_binning.yaml"
    log:
        "logs/phamb/split_contigs.log"
    shell:
        "split_contigs.py -c {input.contigs}; "
        "mkdir -p {params.out}; "
        "mv {params.asmbl} {params.contigs} {params.length} {params.sam_tab} {params.out}"

rule PRODIGAL:
    input:
        sam_tab= "../results/binning/phamb/sample_table.txt" 
    output:
        proteins = "../results/binning/phamb/sample_annotation/{sample}/{sample}.predicted_proteins.faa",
        genes = "../results/binning/phamb/sample_annotation/{sample}/{sample}.predicted_proteins.fna"
    params:  
        tmp_contigs = "sample_annotation/{sample}.unzipped_contigs.fna",
        contigs = "../results/binning/phamb/assembly/{sample}/{sample}" + CONTIGSUFFIX
    threads: 1
    conda:
        "envs/prodigal.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 8000,
        runtime= "1800"
    log:
        "logs/phamb/prodigal/{sample}.prodigal.log"
    shell:
        """
        if [[ {params.contigs} = *.gz ]]; then
            gunzip -c {params.contigs} > {params.tmp_contigs}
            prodigal -i {params.tmp_contigs} -d {output.genes} -a {output.proteins} -p meta -g 11 -q 2>{log}
            rm {params.tmp_contigs}
        else
            prodigal -i {params.contigs} -d {output.genes} -a {output.proteins} -p meta -g 11 -q 2>{log}
        fi
        """

rule miComplete:
    input:
        proteins = "../results/binning/phamb/sample_annotation/{sample}/{sample}.predicted_proteins.faa",
        DB = MICOMPLETEDB
    output:
        hmmfile = "../results/binning/phamb/sample_annotation/{sample}/{sample}.hmmMiComplete105.tbl"
    params:
        tmpoutput = "../results/binning/phamb/sample_annotation/{sample}/{sample}.micomplete.tmp"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 8000,
        runtime= "7200"
    threads: 5
    conda:
        "envs/hmmer.yaml"
    log:
        "logs/phamb/hmm/{sample}.micomplete.log"
    shell:
        "hmmsearch --cpu {threads} -E 1.0e-05 -o {params.tmpoutput} --tblout {output.hmmfile} {input.DB} {input.proteins} 2>{log}"

rule VOG:
    input:
        proteins = "../results/binning/phamb/sample_annotation/{sample}/{sample}.predicted_proteins.faa",
        DB = VOGDB
    output:
        hmmfile = "../results/binning/phamb/sample_annotation/{sample}/{sample}.hmmVOG.tbl"
    params:
        tmpoutput = "../results/binning/phamb/sample_annotation/{sample}/{sample}.hmmVOG.tmp"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "14400"
    threads: 15
    conda:
        "envs/hmmer.yaml"
    log:
        "logs/phamb/hmm/{sample}.VOG.log"
    shell:
        "hmmsearch --cpu {threads} -E 1.0e-05 -o {params.tmpoutput} --tblout {output.hmmfile} {input.DB} {input.proteins} 2>{log}"

rule rename_deepvirfinder:
    input:
        dvf = "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score.tsv/"
    output:
        dvf_renamed = "../results/binning/phamb/sample_annotation/{sample}/{sample}_dvf/{sample}.fna_gt2000bp_dvfpred.txt"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 2000,
        runtime= "1800"

    log:
        "logs/phamb/DVF/{sample}.dvf.log" 
    shell:
        '''
        awk 'BEGIN{{OFS=FS="\t"}}{{sub(/ .*/, "", $1)}}1' {input.dvf} | awk 'BEGIN {{FS="\t"}} $2>2000 {{print}}' | sed '1!s/^/{wildcards.sample}C_/' > {output.dvf_renamed} 
        '''


rule aggergate_anno:
    input:
        anno=get_vids
    output:
        out_anno = directory("../results/binning/phamb/all_annotations")
    params: 
        anno="../results/binning/phamb/sample_annotation"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "7200"
    conda:
        "envs/v_binning.yaml"
    log:
        "logs/phamb/QC/RF.log"
    shell:
        "mkdir -p {output.out_anno}; "
        "cat {params.anno}/*/*hmmMiComplete105.tbl > {output.out_anno}/all.hmmMiComplete105.tbl; "
        "cat {params.anno}/*/*hmmVOG.tbl > {output.out_anno}/all.hmmVOG.tbl; "
        "cat {params.anno}/*/*_dvf/*dvfpred.txt > {output.out_anno}/DVF.predictions.txt; "
        "head -n1 {output.out_anno}/DVF.predictions.txt > {output.out_anno}/DVF.header; "
        "grep -v 'pvalue' {output.out_anno}/DVF.predictions.txt > {output.out_anno}/DVF.predictions; "
        "cat {output.out_anno}/DVF.header {output.out_anno}/DVF.predictions > {output.out_anno}/all.DVF.predictions.txt"


rule RF_QC: # TODO I had to copy the phamb git (https://github.com/RasmussenLab/phamb.git) dbs directory in corresponding .snakemake/conda/
    input:
        contigs = "../data/assembly/concat_assembly/all_concat_assemblies.fasta",
        vamb_dir = "../results/binning/vamb/",
        anno="../results/binning/phamb/all_annotations"
    output:
        out_rf = directory("../results/binning/phamb/QC/")
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "7200"
    conda:
        "envs/v_binning.yaml"
    log:
        "logs/phamb/QC/RF.log"
    shell:
        "run_RF.py {input.contigs} {input.vamb_dir}/vae_clusters.tsv {input.anno} {output.out_rf}"




###############################################################################
################################# Phages fasta extraction #####################
###############################################################################
# This part of the pipeline extracts phages (both bins and contigs) from the 
# metagenome. Single contig bins identified as phages are added to the contigs 
# bins and a score is caclulated with all the identification tools. Vamb multiple
# contigs identified as phage by bins are extracted separately. The bins are
# concat together so that each bins in made of a unique sequence.
#   (1) Identify phages using score and phamb bins
#   (2) Get phages fasta
#       (2.1) Get phages contig fasta
#       (2.2) Get phages bins fasta
#       (2.3) concat all phages together
###############################################################################

##################################### (1) ######################################
rule get_phage_id:
    input:
        vamb="../results/binning/vamb/vae_clusters.tsv",
        phamb ="../results/binning/phamb/QC/vambbins_RF_predictions.txt",
        score = "../results/identification/summary/all_sample_summary_score.tsv"
    output:
        bins = "../results/binning/binning_summary.tsv",
        phages_cntgs = "../results/identification/phages_contigs.tsv",
        phages_bins = "../results/binning/phages_bins.tsv"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 2000,
        runtime= "1800"
    conda:
        "envs/biopython.yaml"
    log:
        "logs/identification/phages_id/phages_id.log"
    script:
        "scripts/identification/get_phage_id.py"


##################################### (2.1) ######################################
rule get_phage_fasta_cntg:
    input:
        phages_id = "../results/identification/phages_contigs.tsv",
        concat_assembly = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"
    output:
        "../data/phages/phages_contigs.fasta/"
    params:
        confidence_thres = 1
    log:
        "logs/phages/phages_fasta/phages_contigs.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/identification/get_phage_cntgs_fasta.py"


##################################### (2.2) ######################################

rule get_phage_fasta_bin:
    input:
        phages_id = "../results/binning/phages_bins.tsv",
        concat_assembly = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"
    output:
        "../data/phages/phages_bins.fasta/"
    log:
        "logs/phages/phages_fasta/phages_bins.log"
    conda:
        "envs/biopython.yaml"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 2000,
        runtime= "3600"
    script:
        "scripts/binning/get_phage_fasta_bin.py"

##################################### (2.3) ######################################


rule cat_all_phages:
    input:
        "../data/phages/phages_bins.fasta",
        "../data/phages/phages_contigs.fasta"
    output:
        "../data/phages/all_phages.fasta"
    log:
        "logs/phages/phages_fasta/cat_all_phages.log"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 2000,
        runtime= "1800"
    shell:
        "cat {input} > {output}"


###############################################################################
################################# Phages quality check #########################
###############################################################################
# This part of the pipeline checks the quality of the phages extract using
# checkv, to assess quality and completeness, and remove host sequqnces for
# proviruses. Then using checkm to remove sequences that are likely of 
# bacterial origin. :
#   (1) Check phages quality using checkv
###############################################################################


##################################### (1) ######################################
# TODO I had install the database and install the remaining file using: diamond makedb --in checkv_reps.faa --db checkv_reps



rule run_checkv:
    input:
        "../data/phages/all_phages.fasta/"
    output:
        outdir = temp(directory("../scratch_link/checkv/phages/")),
        summary = "../results/phages/checkv/quality_summary.tsv",
        provir = "../results/phages/checkv/proviruses.fasta"
    params:
        "../resources/tools/checkv/checkv-db-v1.5"
    log:
        "logs/phages/checkv/phages.log"
    conda:
        "envs/checkv.yaml"
    threads: 15
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 50000,
        runtime= "14400"
    shell:
        "checkv end_to_end {input} {output.outdir} -t {threads} -d {params};"
        "mv {output.outdir}/quality_summary.tsv {output.summary};"
        "mv {output.outdir}/proviruses.fna {output.provir}"


rule trim_prophages:
    input:
        phages = "../data/phages/all_phages.fasta",
        provir = "../results/phages/checkv/proviruses.fna"
    output:
        "../data/phages/all_phages_trimmed.fasta"
    log:
        "logs/phages/checkv/trim_proviruses.log"
    conda:
       "envs/biopython.yaml"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 5000,
        runtime= "1800"
    script:
        "scripts/phages/trim_provirus.py"
    
rule run_checkm:
    input:
        "../data/phages/all_phages_trimmed.fasta"
    output:
        checkm = temp(directory("../data/phages/checkm/")),
        outdir = directory("../scratch_link/checkm/phages/"),
        summary = "../results/phages/checkm/bin_stats_ext.tsv"
    log:
        "logs/phages/checkm/phages.log"
    conda:
        "envs/checkm.yaml"
    threads: 15
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 50000,
        runtime= "36000"
    shell:
        """
        mkdir -p {output.checkm}
        awk '/^>/ {{OUT="{output.checkm}/"substr($0,2) ".fasta"}}; {{print >> OUT; close(OUT)}}' {input}
        checkm lineage_wf {output.checkm} {output.outdir} -t {threads} -x fasta
        mv {output.outdir}/bin_stats_ext.tsv {output.summary}
        """

rule filter_phages_bacteria:
    input:
        phages = "../data/phages/all_phages_trimmed.fasta",
        checkm = "../results/phages/checkm/bin_stats_ext.tsv"
    output:
        checkm_parsed = "../results/phages/checkm/bin_stats_parsed.tsv",
        phages_filtered = "../data/phages/all_phages_trimmed_filtered.fasta"
    log:
        "logs/phages/checkm/filter_phages.log"
    conda:
        "envs/biopython.yaml"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 5000,
        runtime= "1800"
    script:
        "scripts/phages/filter_phages_bacteria.py"


rule derep_phages:
    input:
        "../data/phages/all_phages_trimmed_filtered.fasta"
    output:
        outdir = directory("../results/phages/derep"),
        derep = "../data/phages/all_phages_trimmed_filtered_derep.fasta"
    log:
        "logs/phages/derep/derep_phages.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 50000,
        runtime_s = 10800
    threads:
        5
    conda:
        "envs/derep.yaml"
    shell:
        "vRhyme -i {input} -t {threads} -o {output.outdir} --derep_only --method longest --derep_id 0.99 --frac 0.9;"
        "mv {output.outdir}/vRyhme_dereplication/vRhyme_derep_longest_all_phages_trimmed_filtered.fa {output.derep}"



############################## Clustering ########################################
# This part of the pipeline uses vcontact to cluster the phages sequences into  
# viral clusters.
###############################################################################

rule run_prodigal:
    input:
        "../data/phages/all_phages_trimmed_filtered_derep.fasta"
    output:
        out= temp("../results/phages/prodigal/proteins.faa"),
        prot="../results/phages/prodigal/all_phages_trimmed_filtered.faa",
        chord="../results/phages/prodigal/prot_coord.gbk"
    log:
        "logs/phages/clustering/prodigal.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 50000,
        runtime_s = 14400
    threads:
        5
    conda:
        "envs/prodigal.yaml"
    shell:
        "prodigal -i {input} -a {output.out} -o {output.chord} -q;"
        "awk '/^>/ {{sub(/ .*/, "", $0)}} 1' {output.out} > {output.prot}"


rule gene_2_genome:
    input:
        "../results/phages/prodigal/all_phages_trimmed_filtered.faa"
    output:
        "../results/phages/prodigal/gene_2_genome.tsv"
    log:
        "logs/phages/clustering/gene_2_genome.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 10000,
        runtime_s = 3600
    threads:
        5
    conda:
        "envs/biopython.yaml"
    shell:
        "scripts/clustering/vcontact2_gene2genome.py -p {input} -o {output} -s 'Prodigal-FAA'"

rule run_vcontact:
    input:
        prot= "../results/phages/prodigal/all_phages_trimmed_filtered.faa",
        g2g="../results/phages/prodigal/gene_2_genome.tsv"
    output:
        directory("../results/phages/vcontact")
    log:   
        "logs/phages/clustering/vcontact.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 36000
    conda:
        "envs/vcontact.yaml"
    threads:
        20
    shell:
        "vcontact2 --raw-proteins {input.prot} --proteins-fp {input.g2g} --db 'ProkaryoticViralRefSeq211-Merged' --output-dir {output} --pcs-mode MCL --vcs-mode ClusterONE "


############################## Bacterial Mags ################################
# This part of the pipeline creates Mags from the bacterial bins binned by vamb
###############################################################################

rule get_bins_fasta:
    input:
        bins= "../results/binning/binning_summary.tsv",
        concat_assembly = "../data/assembly/concat_assembly/all_concat_assemblies.fasta"
    output:
        outdir = temp(directory("../scratch_link/binning/bins"))
    log:
        "logs/bacteria/get_bins_fasta.log"
    conda:
        "envs/biopython.yaml"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 5000,
        runtime= "1800"
    script:
        "scripts/bacteria/get_bins_fasta.py"

rule checkm_bins:
    input:
        "../scratch_link/binning/bins"
    output:
        outdir = temp(directory("../scratch_link/checkm/bacteria/")),
        summary = "../results/bacteria/checkm/bin_stats_ext.tsv"
    log:
        "logs/bacteria/checkm/bacteria.log"
    conda:
        "envs/checkm.yaml"
    threads: 15
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 50000,
        runtime= "36000"
    shell:
        "checkm lineage_wf {input} {output.outdir} -t {threads} -x fasta;"
        "mv {output.outdir}/storage/bin_stats_ext.tsv {output.summary}"

rule get_mags:
    input:
        checkm ="../results/bacteria/checkm/bin_stats_ext.tsv",
        bins = "../scratch_link/binning/bins/"
    output:
        directory("../data/bacteria/mags")
    log:
        "logs/bacteria/get_mags.log"
    conda:
        "envs/biopython.yaml"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 5000,
        runtime= "1800"
    script:
        "scripts/bacteria/get_mags.py"

# Taxonomically classify the filtered MAGs
rule classify_gtdbtk:
    input:
        mags="../data/bacteria/mags/"
    output:
        class_out=directory("../results/bacteria/mags/gtdbtk_classification/")
    log:
        "logs/bacteria/mags/gtdbtk_classification.log"
    threads: 8
    conda:
        "envs/gtdbtk.yaml"
    params:
        mash = "../resources/tools/gtdbtk/RefSeqSketchesDefaults.msh/",
        db = "../resources/tools/gtdbtk/release214/"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 150000,
        runtime= "10800"
    shell:
        "export GTDBTK_DATA_PATH={params.db};"
        "gtdbtk classify_wf --genome_dir {input.mags} --extension fasta --out_dir {output.class_out} --mash_db {params.db} --cpus {threads}"

# This rule runs dRep for 95% dereplication
rule run_drep:
    input:
        mags="../data/bacteria/mags/"
    output:
        drep_out=directory("../results/bacteria/mags/drep/"),
        derep_mags= directory("../results/bacteria/mags_derep/")
    log:"logs/bacteria/mags/drep/drep.log"
    threads: 25
    conda:
        "envs/drep.yaml"
    params:
        compl=75
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "14400"
    shell:
        "dRep dereplicate {output.drep_out} -g {input.mags}/* --clusterAlg single --completeness {params.compl} -p {threads};"
        " mv {output.drep_out}/dereplicated_genomes/* {output.derep_mags};"

#################################################################################
############################## HOST ASSIGNATION #################################
#################################################################################
# to assign hosts to every viral contigs we will use the CRISPR spacers and genome homology:
#TODO intall CrispOpenDB

# 1. find CRISPR spacers in all MAGs and refernce bacterial genome
# 2. Map spacers against all viral contigs
# 3. Use fastani to identify genome Homology between viral contigs and reference bacterial genomes
# 4. Use the results of 2 and 3 to assign a host to every viral contigs
#################################################################################

###################################### 1 ########################################
rule find_CRISPR_spacers:
    input:
        mags = expand("../data/bacteria/mags_derep/")
    output:
        spacers_dir=temp("../results/host_assign/spacers_db/hb_spacers/{mags}-spacers")
    log:
        "logs/host_assign/spacers_db/find_spacers_{mags}.log"
    benchmark:
        "logs/host_assign/spacers_db/find_spacers_{mags}.benchmark"
    threads: 1
    conda:
        "envs/singularity.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "14400"
    shell:
        "scripts/host_assign/run_ccf.sh {input} {output} {wildcards.mags}"


rule parse_CCF:
    input:
        all_spacers=expand("../results/host_assign/spacers_db/hb_spacers/{mags}-spacers",
        mags = glob_wildcards("../data/bacteria/mags_derep/{mags}").mags)
    output:
        parsed_crispr=directory("../results/host_assign/spacers_db/hb_spacers_parsed/")
    log:
        "logs/host_assign/spacers_db/parse_CCF.log"
    benchmark:
        "logs/host_assign/spacers_db/parse_CCF.benchmark"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "1800"
    script:
        "scripts/host_assign/CCF_parser.py"

rule create_spacersDB:
    input:
        openDB_spacers="../resources/tools/CrisprOpenDB/CrisprOpenDB/SpacersDB.fasta",
        my_spacers="../results/host_assign/spacers_db/hb_spacers_parsed/"
    output:
        spacers_db=directory("../results/host_assign/spacers_db/spacersDB/")
    log:
        "logs/host_assign/spacers_db/create_spacersDB.log"
    benchmark:
        "logs/host_assign/spacers_db/create_spacersDB.benchmark"
    threads: 1
    conda:
        "envs/blast.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "3600"
    shell:
        "mkdir -p {output.spacers_db}; "
        "cat {input.my_spacers}/CRISPR_v1.fasta {input.openDB_spacers} > {output.spacers_db}/myspacersDB.fasta; "
        "makeblastdb -in {output.spacers_db}/myspacersDB.fasta -dbtype nucl -out {output.spacers_db}/mySpacersDB"

# ###################################### 2 ########################################
rule assign_host:
    input:
        phageDB="../data/phages/all_phages_trimmed_filtered_derep.fasta",
        spacers_db="../results/host_assign/spacers_db/spacersDB/"
    output:
        "../results/host_assign/spacers/CrispRopenDB_report.tsv"
        # blastout=temp("../results/host_assign/{host/spacers_{blastout.txt"),
        # spacers_report=temp("../results/host_assign/host/CRISPRopenDB_report.txt")
    log:
        "logs/host_assign/assign_host/assign_host.log"
    conda:
        "envs/CrisprOpenDB.yaml"
    params:
        DB="resources/tools/CrisprOpenDB"
    threads: 10
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "36000"
    shell:
        "python ../resources/tools/CrisprOpenDB/CL_Interface.py -i {input.phageDB} -u -b  {input.spacers_db}/mySpacersDB -m 2 -n {threads} -t > {output}"

        # "wd=$(pwd -P); "
        # "scripts/host_assign/run_assign_host.sh ${{wd}} {input.phageDB} {input.spacers_db} {output.blastout} {output.spacers_report} {threads} {wildcards.sample}"


# rule aggregate_assign_host:
#     input:
#         blastout=expand("../results/host_assign/{sample}_host/spacers_{sample}_blastout.txt", sample=config["samples"]),
#         spacers_report=expand("../results/host_assign/{sample}_host/CRISPRopenDB_{sample}_report.txt", sample=config["samples"])
#     output:
#         all_blastout="../results/host_assign/spacers/all_spacers_blastout.txt",
#         all_spacers_report="../results/host_assign/spacers/all_CRISPRopenDB_report.txt"
#     log:
#         "logs/host_assign/aggregate_assign_host.log"
#     threads: 1
#     conda:
#         "envs/CrisprOpenDB.yaml"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 100000,
#         runtime= "1800"
#     shell:
#         "tail -n +2 -q {input.spacers_report}> {output.all_spacers_report}; "
#         "echo -e 'Hit_nr,SPACER_ID,Query,identity,alignement_length,mismatch,gap,q_start,q_end,s_start,s_end,e_value,score,GENEBANK_ID,ORGANISM_NAME,SPECIES,GENUS,FAMILY,ORDER,SPACER,SPACER_LENGTH,COUNT_SPACER,POSITION_INSIDE_LOCUS,true_num_mismatch' > {output.all_blastout}; "
#         "tail -n +2 -q {input.blastout} >> {output.all_blastout}; "

# ###################################### 3 ########################################
rule fastani_prophages:
    input:
        mags="../data/bacteria/mags_derep/",
        viruses="../data/phages/all_phages_trimmed_filtered_derep.fasta"
    output:
        "../results/host_assign/prophages/fastani_out.txt"
    threads: 15
    params:
        outdir="../scratch_link/viruses/",
        mags_l="./mags_l.txt",
        vir_l="./vir_l.txt"
    log:
        "logs/host_assign/prophages/fastani.log"
    conda:
        "envs/drep.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "7200"
    shell:
        '''
        if [ ! -d {params.outdir} ]; then
            mkdir {params.outdir};
            awk '/^>/ {{OUT="{params.outdir}" substr($0,2) ".fa"}}; {{print >> OUT; close(OUT)}}'  "{input.viruses}";
        fi;
        find {input.mags} -maxdepth 1 -type f -not -name '.*' -printf '{input.mags}/%f\n' > {params.mags_l}; 
        find {params.outdir} -maxdepth 1 -type f -not -name '.*' -printf '{params.outdir}/%f\n' > {params.vir_l}; 
        fastANI --ql {params.vir_l} --rl {params.mags_l} -t {threads} --fragLen 3000 -o {output}; 
        #rm {params.mags_l} {params.vir_l}
        '''

# ###################################### 4 ########################################
rule parse_host_assignation:
    input:
        all_blastout="../results/host_assign/spacers/all_spacers_blastout.txt",
        pro="../results/host_assign/prophges/fastani_out.txt",
        spacers_mtdata="../results/spacers_db/hb_spacers_parsed",
        clust_filtered="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv",
        binning_data="../results/vMAG_binning/summary_table/aggregate_filtering_data.tsv"
    output:
       formatted_spacers="../results/host_assigniation/summary_table/spacers_metadata.tsv",
       blastout_filt_complete="../results/host_assigniation/summary_table/spacers_blastout_filt_complete.tsv",
       blastout_formatted="../results/host_assigniation/summary_table/spacers_blastout_filt_formatted.tsv",
       spacers_host="../results/host_assigniation/summary_table/spacers_host.tsv",
       prophages_host="../results/host_assigniation/summary_table/prophages_host.tsv",
       phage_host="../results/host_assigniation/summary_table/phage_host.tsv"
    log:
        "logs/host_assign/parse_host_assignation.log"
    conda:
        "envs/base_R_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "00:30:00"
    script:
        "scripts/assign_host/parse_host_assignation.R"