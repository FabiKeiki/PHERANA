configfile: "../config/config.yaml"

rule all:
    input:
        expand("../data/phages/vsearch/{sample}_phages_contigs_2_dereplicated.fasta",sample=config['samples'])

#################### Main Functions #######################################
import os
def get_direction_r1(wildcards) :
#This  function returns a list of R1 reads when the same sample was sequenced on multiple lanes
    samname=wildcards.sample
    l=config["samples"][samname]

    r_list=[]
    r1_l1=[s for s in l if "L1_R1" in s]
    r1_l2=[s for s in l if "L2_R1" in s]
    r1_l3=[s for s in l if "L3_R1" in s]
    r1_l4=[s for s in l if "L4_R1" in s]

    r_list.extend(r1_l1)
    r_list.extend(r1_l2)
    if len(r1_l3) >0: r_list.extend(r1_l3)
    if len(r1_l4) >0: r_list.extend(r1_l4)
    return(r_list)


def get_direction_r2(wildcards) :
#This  function returns a list of R2 reads when the same sample was sequenced on multiple lanes
    samname=wildcards.sample
    l=config["samples"][samname]

    r_list=[]
    r2_l1=[s for s in l if "L1_R2" in s]
    r2_l2=[s for s in l if "L2_R2" in s]
    r2_l3=[s for s in l if "L3_R2" in s]
    r2_l4=[s for s in l if "L4_R2" in s]

    r_list.extend(r2_l1)
    r_list.extend(r2_l2)
    if len(r2_l3) >0: r_list.extend(r2_l3)
    if len(r2_l4) >0: r_list.extend(r2_l4)
    return(r_list)



################################################################################
########################### DATA VALIDATION ####################################
################################################################################
# The following pipeline is used to ascertain that the metagenomics samples
# are composed of what we expect (viruses in the virome fraction, bacteria in the
# bacteriome one)
################################################################################
################################################################################


################################### Concat Lanes ###################################


#Concatenate raw reads from the same sample that were sequenced on different lanes
rule concat_lanes:
    input:
        R1s=get_direction_r1,
        R2s=get_direction_r2
    output:
        R1_concat="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz", # these files go in scratch because they can be quickly recreated if needed
        R2_concat="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz"
    threads: 2
    log:
        "logs/data_validation/lane_concatenation/{sample}_concat.log"
    resources:
        account = "pengel_beemicrophage",
        runtime_s = 7200
    shell:
        "cat {input.R1s} > {output.R1_concat}; cat {input.R2s} > {output.R2_concat}; "


################################### Kraken2 ###################################

# This rule takes the concatenated raw reads files and runs them against the krakend db
rule run_kraken:
    input:
        R1="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        R2="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz",
        db="../../../mndiaye1/PHOSTER/workflow/resources/default_DBs/230228_costum_kraken2db_new"
    output:
        tab=temp("../results/data_validation/kraken2_output/{sample}_kraken2_report.kraken"),
        rep="../results/data_validation/kraken2_output/Reports/{sample}_kraken2_report",
    conda:
        "envs/Kraken2.yaml"
    threads: 24
    log:
        "logs/data_validation/kraken2/run/{sample}_kraken2.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime_s= 10800
    shell:
        "kraken2 --use-names --threads {threads} \
         --db {input.db} \
         --fastq-input --report {output.rep}  --gzip-compressed \
         --paired {input.R1} {input.R2} \
         > {output.tab}"
    
# This rule parses the kraken output for further analyes
rule parse_kraken_report:
    input:
        "scripts/data_validation/parse_kraken_report.py", # if you change the script, the rule runs again
        expand("../results/data_validation/kraken2_output/Reports/{sample}_kraken2_report", sample=config["samples"])
    output:
        "../results/data_validation/kraken2_output/Summary/all_samples_report.txt"
    threads: 2
    params:
        "../results/data_validation/kraken2_output/"
    log:
        "logs/data_validation/kraken2/parsing/report_parser_kraken2.log"
    resources:
        account = "pengel_beemicrophage",
        runtime_s= 1000
    script:
        "scripts/data_validation/parse_kraken_report.py"



############################# QC and Trimming ##################################

# This rule does a fastQC on the raw reads
rule fastQC_PreTrimming:
    input:
        R1=get_direction_r1,
        R2=get_direction_r2
    output:
        temp(directory("../results/data_validation/QC/preTrimming/QC_{sample}/"))
    threads: 2
    log:
        "logs/data_validation/QC/{sample}_QC.log"
    conda:
        "envs/fastqc.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime_s= 3600
    shell:
        "mkdir -p {output}; "
        "fastqc -o {output} {input.R1};"
        "fastqc -o {output} {input.R2};"


# This rule runs the trimming of the raw reads
rule rawreads_trimming:
    input:
        R1="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        R2="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz"
    output:
        R1_paired="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2_paired="../data/trimmed_reads/{sample}_R2_paired.fastq.gz",
        R1_unpaired="../data/trimmed_reads/{sample}_R1_unpaired.fastq.gz",
        R2_unpaired="../data/trimmed_reads/{sample}_R2_unpaired.fastq.gz"
    threads: 8
    params:
        nextera="../data/reference_assemblies/short_RefSeqs/NexteraPE-PE.fa",
        q=28,
        min_length=40
    log:
        "logs/data_validation/trimming/{sample}_trimming.log"
    conda:
        "envs/trimmomatic.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime_s= 10800
    shell:
        "trimmomatic PE -phred33 -threads {threads} {input.R1} {input.R2} \
         {output.R1_paired} {output.R1_unpaired} {output.R2_paired} {output.R2_unpaired} \
         ILLUMINACLIP:{params.nextera}:2:30:10 \
         LEADING:{params.q} TRAILING:{params.q} MINLEN:{params.min_length}"

# this rule does a post trimming fast QC
rule fastQC_PostTrimming:
    input:
        R1="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2="../data/trimmed_reads/{sample}_R2_paired.fastq.gz"
    output:
        temp(directory("../results/data_validation/QC/postTrimming/QC_{sample}/"))
    threads: 2
    log:
        "logs/data_validation/QC/{sample}_postQC.log"
    conda:
        "envs/fastqc.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime_s= 3600
    shell:
        "mkdir -p {output}; "
        "fastqc -o {output} {input.R1} {input.R2}"

rule parse_fastQC:
    input:
        preT=expand("../results/data_validation/QC/preTrimming/QC_{sample}/", sample=config["samples"]),
        postT=expand("../results/data_validation/QC/postTrimming/QC_{sample}/", sample=config["samples"])
    output:
        "../results/data_validation/QC/Summary/fastQC_summary.txt"
    threads: 2
    log:
        "logs/data_validation/QC/summarize_fastQC.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime= 1800
    script:
        "scripts/data_validation/parse_fastqc_output.py"


############################### Host Filtering #################################

# This rule uses bbsplit to map remove reads from the honeybee genome and/or the human genomes
rule host_filtering:
    input:
        R1="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2="../data/trimmed_reads/{sample}_R2_paired.fastq.gz"
    output:
        dir=temp(directory("../results/data_validation/host_filtering/discarded/{sample}_discarded/")), # I don't need the sam of the mapping so I delete them immediatly
        unmapped_R1="../data/host_filtered_reads/{sample}_R1_HF.fastq.gz", # these are the filtered reads
        unmapped_R2="../data/host_filtered_reads/{sample}_R2_HF.fastq.gz",
        refstats="../results/data_validation/host_filtering/HF_mappings_stats/{sample}_refstats.out"
    conda:
        "envs/bwa_mapping.yaml"
    threads: 25
    params:
        ref_Acer="../data/reference_assemblies/A_cerana/GCF_001442555.1_ACSNU-2.0_genomic.fna.gz",
        ref_Hsap="../data/reference_assemblies/H_sapiens/GCF_000001405.40_GRCh38.p14_genomic.fna.gz",
        xmx="50g"
    log:
        "logs/data_validation/HF/{sample}_HF.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime_s= 28800
    shell:
        "bbsplit.sh in1={input.R1} in2={input.R2} ref={params.ref_Acer},{params.ref_Hsap} \
        basename={output.dir}/{wildcards.sample}_HF_discarded_%.sam \
        refstats={output.refstats} rebuild=t nodisk=t \
        outu1={output.unmapped_R1} outu2={output.unmapped_R2} nzo=f -Xmx{params.xmx} threads={threads}"


# This rule parses the refstats output of the filering
rule parse_filtering_refstats:
    input:
        files=expand("../results/data_validation/host_filtering/HF_mappings_stats/{sample}_refstats.out", sample=config["samples"])
    output:
        "../results/data_validation/host_filtering/HF_mappings_stats/HF_refstats.txt"
    threads: 2
    log:
        "logs/data_validation/HF/HF_refstats_parsing.log"
    params:
        file1="../results/data_validation/host_filtering/HF_mappings_stats/file1.txt",
        file2="../results/data_validation/host_filtering/HF_mappings_stats/file2.txt",
        tmp="../results/data_validation/host_filtering/HF_mappings_stats/tmp.txt",
        sams=expand("{sample}", sample=config["samples"])
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500,
        runtime_s= 1800
    shell:
        "echo -e 'sample\tname\tperc_unambiguousReads\tunambiguousMB\tperc_ambiguousReads\tambiguousMB\tunambiguousReads\tambiguousReads\tassignedReads\tassignedBases' > {output}; "
        "tail -n +2 -q {input.files} > {params.file1}; "
        "printf '%s\n' {params.sams} > {params.file2}; "
        "awk '{{for(i=0;i<2;i++)print}}' {params.file2} > {params.tmp}; "
        "paste -d '\t' {params.tmp} {params.file1} >> {output}"


############################### count_reads ##################################
# After trimming and host filtering the reads, I wanna know how much I lost in
# Terms of reads and bases


# this rules returns a table of read count before and after trimming
rule count_reads_qc:
    input:
        preT="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        postT="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        postF="../data/host_filtered_reads/{sample}_R1_HF.fastq.gz"
    output:
        temp("../results/data_validation/QC/{sample}_read_count.txt")
    log:
        "logs/data_validation/QC/read_count/{sample}_read_count.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 4000,
        runtime_s= 1800
    shell:
        "touch {output}; "
        "(./scripts/data_validation/count_reads.sh {input.preT} {input.postT} {input.postF} {output})2>{log}"


rule count_reads_summary:
    input:
        sams=expand("../results/data_validation/QC/{sample}_read_count.txt", sample=config["samples"])
    output:
        "../results/data_validation/QC/Summary/read_count.txt"
    log:
        "logs/data_validation/QC/read_count/summary_read_count.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 4000,
        runtime_s= 1800
    shell:
        "(awk 'FNR!=NR && FNR==1 {{next}} 1' {input.sams} > {output})2>{log}"



################################################################################
###############################  mOTUS  ########################################
################################################################################
# mOTUs is more has more taxonomical resolution than kraken, so once the reads
# are all cleaned and filtered, I run mOTUs to obtain a genus-level composition
# of the remaining reads (for the bacterial samples)
################################################################################
################################################################################

# this rule runs the motu profiling
# it is better to launch this rule with one sample first and then all the others,
# mOTUs db download doesn't handle weell multiple files trying to download it and
# access it at the same time and the jobs might fail.
rule run_motus:
    input:
        reads1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        reads2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        motus_temp = temp("../results/data_validation/motus_output/map/{sample}_map.motus")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime_s= 9000
    threads: 24
    log:
        "logs/data_validation/motus/{sample}_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus downloadDB; " # just leave it here to be safe - it warns and continues
        "motus profile -f {input.reads1} -r {input.reads2} -n {wildcards.sample} -o {output.motus_temp} -t {threads}"

# this roule run the mouts counting of the marker genes that map to the datbase
rule run_motus_count:
    input:
        reads1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        reads2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        motus_temp = temp("../results/data_validation/motus_output/count/{sample}_count.motus")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime_s= 9000
    threads: 24
    log:
        "logs/data_validation/motus/{sample}_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus downloadDB; " # just leave it here to be safe - it warns and continues
        "motus profile -f {input.reads1} -r {input.reads2} -c -n {wildcards.sample} -o {output.motus_temp} -t {threads}"

rule merge_motus:
    input:
        motus_temp = expand("../results/data_validation/motus_output/map/{sample}_map.motus", sample=config["samples"]),
        motus_count= expand("../results/data_validation/motus_output/count/{sample}_count.motus", sample=config["samples"])
    output:
        motus_merged = "../results/data_validation/motus_output/Summary/samples_merged_map.motus",
        mouts_merged_count = "../results/data_validation/motus_output/Summary/samples_merged_count.motus"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 500000,
        runtime= 7200
    threads: 12
    log:
        "logs/data_validation/motus/merge_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus merge -a bee -i $(echo \"{input.motus_temp}\" | sed -e 's/ /,/g' ) > {output.motus_merged}; "
        "motus merge -a bee -c -i $(echo \"{input.motus_count}\" | sed -e 's/ /,/g' ) > {output.mouts_merged_count}"

rule parse_motus:
    input:
        motus_tab = "../results/data_validation/motus_output/Summary/samples_merged_map.motus",
        motus_count = "../results/data_validation/motus_output/Summary/samples_merged_count.motus"
    output:
        "../results/data_validation/motus_output/Summary/motus_combined.txt"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 50000,
        runtime_s= 14400
    log:
        "logs/data_validation/motus/parse_motus.log"
    conda:
        "envs/base_R_env.yaml"
    script:
        "scripts/data_validation/parse_motus.R"



################################################################################
################################## Sample Assembly #############################
################################################################################
# This part of the pipleine takes takes host filtered reads (both virome and bacterial) and reconstructs MAGS
# This part of the pipeline is divided in several steps:
#   (1): Assembly : Using different assemblers and comparing them with quast
#   (1.1) metaSPades
#   (1.2) metaviralSPades
#   (1.3) MEGAHIT
#   (1.4) stats on assemblies with metquast
#   (1.5) filtering assemblies
#   (1.6) stats on filtered assemblies with metaquast
################################################################################
################################################################################

##################################### (1.1) ######################################
# the following rules use metaSPades to assemble the metagenomes of every sample.

rule assemble_metaspades:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scratch_dir=directory("../scratch_link/assembly/metaspades/{sample}_metaspades/"),
        contigs="../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs.fasta",
    params:
        memory_limit = 200,
    threads: 40
    resources:
        account="pengel_beemicrophage",
        mem_mb= 250000,
        runtime_s= 18000
    log:
        "logs/assembly/metaspades/{sample}_assemble_HF.log"
    conda:
        "envs/assembly.yaml"
    shell:
        "spades.py --meta --pe1-1 {input.R1} --pe1-2 {input.R2} \
        -o {output.scratch_dir} \
        -k 21,33,55,77,99,127 -m {params.memory_limit} -t {threads}; "
        "mv {output.scratch_dir}/contigs.fasta {output.contigs}; "
        

##################################### (1.2) ######################################
# the following rules use metaviralSPades to assemble the metagenomes of every sample.

rule assemble_metaviralspades:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scratch_dir=directory("../scratch_link/assembly/metaviralspades/{sample}_metaviralspades/"),
        contigs="../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs.fasta"
    params:
        memory_limit = 200
    threads: 40
    resources:
        account="pengel_beemicrophage",
        mem_mb= 250000,
        runtime_s= 18000
    log:
        "logs/assembly/metaviralspades/{sample}_assemble.log"
    conda:
        "envs/assembly.yaml"
    shell:
        "spades.py --metaviral --pe1-1 {input.R1} --pe1-2 {input.R2} \
        -o {output.scratch_dir} \
        -k 21,33,55,77,99,127 -m {params.memory_limit} -t {threads}; "
        "mv {output.scratch_dir}/contigs.fasta {output.contigs}; "
        
        
##################################### (1.3) ######################################
# the following rules use MEGAHIT to assemble the metagenomes of every sample.

rule assemble_megahit:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scratch_dir=directory("../scratch_link/assembly/megahit/{sample}_megahit/"),
        contigs="../data/assembly/megahit/{sample}_megahit/{sample}_megahit_contigs.fasta"
    params:
        dir=directory("../scratch_link/assembly/megahit/{sample}_megahit/")
    threads: 20
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s= 15000
    log:
        "logs/assembly/megahit/{sample}_assemble.log"
    conda:
        "envs/assembly.yaml"
    shell:
        "megahit -1 {input.R1} -2 {input.R2} \
        -o {output.scratch_dir} \
        --k-min 21 --k-max 127 --k-step 12;" 
        "mv {output.scratch_dir}/final.contigs.fa {output.contigs}; "
       


##################################### (1.4) ######################################
# the following rules use metaquast to retrieve statistics for each assemblers


rule run_quast_contigs:
    input:
        mspades= "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs.fasta",
        mvspades= "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs.fasta",
        megahit = "../data/assembly/megahit/{sample}_megahit/{sample}_megahit_contigs.fasta"
    output:
        dir=directory("../scratch_link/assembly/quast/{sample}_quast_output/"),
        report= "../results/assembly/quast/{sample}_quast_report.tsv"
        
    log:
        "logs/assembly/quast/{sample}_quast.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 25000,
        runtime_s =  3600
    conda:
        "envs/quast_contigs.yaml"
    shell:
        "metaquast.py {input.mspades} {input.mvspades} {input.megahit} -o {output.dir} --max-ref-number 0;"
        "mv {output.dir}/transposed_report.tsv {output.report}"



rule parse_quast_reports:
    input:
       expand("../results/assembly/quast/{sample}_quast_report.tsv",sample=config["samples"])
    output:
        "../results/assembly/quast/summary_quast_report.tsv"
    log:
        "logs/assembly/quast/summary_quast.log"
    resources:
        account="pengel_beemicrophage",
        runtime_s = 1800
    script:
        "scripts/assembly/parse_quast_report.py"

##################################### (1.5) ######################################
# THe follwoing rules apply a filter to the assembled contigs

rule filter_assembly:
    input:
        expand("../data/assembly/{assembly}/{sample}_{assembly}/{sample}_{assembly}_contigs.fasta",sample=config["samples"],assembly=config["assembly"])
    output:
        "../data/assembly/{assembly}/{sample}_{assembly}/{sample}_{assembly}_contigs_filt.fasta"
    params:
        length_t = 1000,
        cov_t = 1
    resources:
        account="pengel_beemicrophage",
        mem_mb= 2000,
        runtime_s= 1800
    log:
        "logs/assembly/filtering/{assembly}/{sample}_filtering.log"
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/assembly/filter_assembly.py"
##################################### (1.6) ######################################
# the following rules use metaquast to retrieve statistics for each filtered assemblies


rule run_quast_contigs_filt:
    input:
        mspades= "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs_filt.fasta",
        mvspades= "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs_filt.fasta",
        megahit = "../data/assembly/megahit/{sample}_megahit/{sample}_megahit_contigs_filt.fasta"
    output:
        dir=directory("../scratch_link/assembly/quast/{sample}_quast_output_filt/"),
        report= "../results/assembly/quast/{sample}_quast_report_filt.tsv"
        
    log:
        "logs/assembly/quast/{sample}_quast_filt.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 25000,
        runtime_s =  3600
    conda:
        "envs/quast_contigs.yaml"
    shell:
        "metaquast.py {input.mspades} {input.mvspades} {input.megahit} -o {output.dir} --max-ref-number 0;"
        "mv {output.dir}/transposed_report.tsv {output.report}"



rule parse_quast_reports_filt:
    input:
       expand("../results/assembly/quast/{sample}_quast_report_filt.tsv",sample=config["samples"])
    output:
        "../results/assembly/quast/summary_quast_report_filt.tsv"
    log:
        "logs/assembly/quast/summary_quast_filt.log"
    resources:
        account="pengel_beemicrophage",
        runtime_s = 1800
    script:
        "scripts/assembly/parse_quast_report.py"



################################################################################
############################## Phage identification  ###########################
################################################################################
#This part of the pipeline runs three tools on all the assembly, to identify 
#phages, these sequences will then dereplicated and binned. Each phages id tools
#ouput are parsed and merged, and a fasta file of the phages sequences is extracted


rule concat_assembly:
    input: 
        mspades= "../data/assembly/metaspades/{sample}_metaspades/{sample}_metaspades_contigs_filt.fasta",
        mvspades= "../data/assembly/metaviralspades/{sample}_metaviralspades/{sample}_metaviralspades_contigs_filt.fasta",
        megahit = "../data/assembly/megahit/{sample}_megahit/{sample}_megahit_contigs_filt.fasta"
    output:
        concat_assembly = "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    log:
        "logs/assembly/concat_assembly/{sample}_concat.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 1000,
        runtime_s = 7200
    threads:
        5
    shell:
        "cat {input.mspades} {input.mvspades} {input.megahit} > {output.concat_assembly}"
    
rule run_virsorter:
    input:
       "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = directory("../scratch_link/identification/virsorter/{sample}_virsorter/"),
        score = "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score.tsv/"
    params:
        db = directory("../scratch_link/virsorter/")
    log:
        "logs/identification/virsorter/{sample}_virsorter.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/virsorter2.yaml"
    shell:
        "virsorter setup -d {params.db} -j 40;"
        "virsorter run -w {output.outdir} -i {input} --keep-original-seq  -j 40 all;"
        "mv {output.outdir}/final-viral-score.tsv {output.score}"

rule run_viralverify:
    input:
        "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = directory("../scratch_link/identification/viralverify/{sample}_viralverify/"),
        score = "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score.csv/"
    params:
        hmm = "../data/hmms/nbc_hmms.hmm.gz"
    log:
        "logs/identification/viralverify/{sample}_viralverify.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/viralverify.yaml"
    shell:
        "viralverify -f {input} -o {output.outdir} --hmm {params.hmm} -t {threads};"
        "mv {output.outdir}/{wildcards.sample}_concat_assembly_result_table.csv {output.score}"

rule run_deepvirfinder:
    input:
         "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = directory("../scratch_link/identification/deepvirfinder/{sample}_deepvirfinder"),
        score = "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score.tsv/"
    log:
        "logs/identification/deepvirfinder/{sample}_dvfinder.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/deepvirfinder.yaml"
    shell:
        "dvf.py -i {input} -o {output} -c {threads};"
        "mv {output.outdir}/{wildcards.sample}_concat_assembly.fasta_gt1bp_dvfpred.txt {output.score}"

rule download_vibrant_db:
    output:
        directory("vibrant_db")
    log:
        "logs/identification/vibrant/download_db.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/vibrant.yaml"
    shell:
        "download-db.sh;"
        "mkdir {output}"

rule run_vibrant:
    input:
        assembly = "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta",
        vibrant_db = directory("vibrant_db")
    output:
        outdir = directory("../scratch_link/identification/vibrant/{sample}/"),
        score = "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score.tsv/"
    log:
        "logs/identification/vibrant/{sample}_vibrant.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s = 14400
    threads:
        20
    conda:
        "envs/vibrant.yaml"
    shell:
        "VIBRANT_run.py -i {input.assembly} -folder {output.outdir} -t {threads};"
        "mv {output.outdir}/VIBRANT_{wildcards.sample}_concat_assembly/VIBRANT_phages_{wildcards.sample}_concat_assembly/{wildcards.sample}_concat_assembly.phages_combined.txt {output.score};"

        #   if [ -d vibrant_db ]; then
        #     rm -rf vibrant_db
        # fi
       

rule parse_virsorter:
    input:
        "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score.tsv/"
    output:
        "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score_parsed.tsv/"
    log:
        "logs/identification/virsorter/{sample}_parse_virsoter.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_virsorter.py"

rule parse_viralverify:
    input:
        "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score.csv/"
    output:
        "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score_parsed.tsv/"
    log:
        "logs/identification/viralverify/{sample}_parse_viralverify.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_viralverify.py"

rule parse_deepvirfinder:
    input:
        "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score.tsv/"
    output:
        "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score_parsed.tsv/"
    log:
        "logs/identification/deepvirfinder/{sample}_parse_deepvirfinder.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_deepvirfinder.py"

rule parse_vibrant:
    input:
        "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score.tsv/"
    output:
        "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score_parsed.tsv/"
    log:
        "logs/identification/vibrant/{sample}_parse_vibrant.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_vibrant.py"

rule parse_phage_id:
    input:
        virsorter = "../results/identification/virsorter/{sample}_virsorter/{sample}_virsorter_score_parsed.tsv/",
        viralverify =  "../results/identification/viralverify/{sample}_viralverify/{sample}_viralverify_score_parsed.tsv/",
        deepvirfinder =  "../results/identification/deepvirfinder/{sample}_deepvirfinder/{sample}_deepvirfinder_score_parsed.tsv/",
        vibrant =  "../results/identification/vibrant/{sample}_vibrant/{sample}_vibrant_score_parsed.tsv/"
    output:
        "../results/identification/summary/{sample}_summary_score.tsv/"
    log:
        "logs/identification/summary/{sample}_summary.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    script:
        "scripts/identification/parse_phage_id.py"

rule get_phage_fasta:
    input:
        phages_id = "../results/identification/summary/{sample}_summary_score.tsv/",
        concat_assembly = "../data/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        "../data/phages/{sample}_phages_contigs_{confidence_thres}.fasta/"

    params:
        confidence_thres = 2
    log:
        "logs/identification/phages/{sample}_phages_{confidence_thres}.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 5000,
        runtime_s = 3600
    threads:
        5
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/identification/get_phage_fasta.py"



################################################################################
############################## Derepliction       ##############################
################################################################################

rule dereplicate_assembly:
    input:
        "../data/phages/{sample}_phages_contigs_2.fasta/"
    output:
        cluster_centr = "../data/phages/vsearch/{sample}_phages_contigs_2_dereplicated.fasta",
        cluster_info = "../results/phages/vsearch/{sample}_clusters_2.tsv"
    log:
        "logs/assembly/vsearch/{sample}_vsearch.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime_s =  144000
    threads:
        40
    conda:
        "envs/vsearch.yaml"
    shell:
        "vsearch --cluster_fast {input} --centroids {output.cluster_centr} --uc {output.cluster_info} \
          -id 0.99 -iddef 0 --query_cov 0.9 --maxseqlength 10000000 --threads {threads}"

